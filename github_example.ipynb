{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad257cc-6a5d-4efc-9329-4df7a26fe108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_columns\" , 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a882307-0027-444c-b7ed-7e4de344d783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_url = \"https://git.io/nlp-with-transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7fa047b-987b-40bf-8799-ced5bc2c851e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(dataset_url, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c35205-a9e3-4909-ad64-e0b4528e118a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"url\", \"id\", \"title\", \"user\", \"labels\", \"state\", \"created_at\", \"body\"]\n",
    "df.loc[2, cols].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56ae0ea-6fcd-4afc-bb33-82963c152ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"labels\"] = (df[\"labels\"]\n",
    "                       .apply(lambda x: [meta[\"name\"] for meta in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c977a450-07a2-4305-8a13-a8ea957c3bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"github.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c566cce-275a-40a4-a6ca-8b4a910a084c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Find total number of labels per post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a31f8f0-13d4-4d23-af32-736df458431e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"labels\"].apply(lambda x: len(x)).value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "262793ed-05ac-4eec-8d2b-3402529535fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Find which labels are most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512566cf-f0e8-40b2-a7f2-3a30611a0021",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_counts = df[\"labels\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a869727d-285f-4193-afd0-0683379635a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_counts.to_frame().head(8).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5556506a-60ff-4b88-af8e-c72e7149ce6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### get only the top 8 labels we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae25f8b-ca77-44c3-8d7e-2d4ffa8afa5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "label_map = {\"Core: Tokenization\": \"tokenization\",\n",
    "             \"New model\": \"new model\",\n",
    "             \"Core: Modeling\": \"model training\",\n",
    "             \"Usage\": \"usage\",\n",
    "             \"Core: Pipeline\": \"pipeline\",\n",
    "             \"TensorFlow\": \"tensorflow or tf\",\n",
    "             \"PyTorch\": \"pytorch\",\n",
    "             \"Examples\": \"examples\",\n",
    "             \"Documentation\": \"documentation\"}\n",
    "\n",
    "def filter_labels(x):\n",
    "    return [label_map[label] for label in x if label in label_map]\n",
    "\n",
    "df[\"labels\"] = df[\"labels\"].apply(filter_labels)\n",
    "all_labels = list(label_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1be4706-8e10-4ed6-a219-fed3fced943e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "220249a5-8ad6-42ef-bcd9-3cc7db4246f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_counts = df[\"labels\"].explode().value_counts()\n",
    "df_counts.to_frame().head(8).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e4af8f9-1643-4bd6-b4ea-b81b504ac0b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## create an identifier to split if row has label or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4038d6e-c0a7-4e56-b138-d1de78f44b72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90d3283-7446-441f-8793-22f4ea4c5ad8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"split\"] = \"unlabeled\"\n",
    "mask = df[\"labels\"].apply(lambda x: len(x)) > 0\n",
    "df.loc[mask, \"split\"] = \"labeled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e2696b-4790-4f21-b246-f7cb00f91deb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7fa95c6-92d0-494b-adad-74fa0c117ef8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"text\"] = (df.apply(lambda x: x[\"title\"] + \"\\n\\n\" + x[\"body\"], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc96a7a9-5f68-446d-8e37-2b5617f8401f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a71904-1c2d-4e1a-88de-e9feb84acb3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb36301-4b75-43e1-ae27-83f1be2a99d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len_before = len(df)\n",
    "df = df.drop_duplicates(subset = \"text\")\n",
    "print(f\"Removed {(len_before - len(df))/len_before:.2%} duplicates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68fda934-4c90-4bf3-a94b-5cb257f8046b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2b747b8-4a49-4701-a53b-b79843b43a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdea12d0-e350-4834-8898-056a18c6753b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "MultiLabelBinarizer takes a list of label names and creates a vector with zeros for absent labels and ones for present labels. We can test this by fitting MultiLabelBinarizer on all_labels to learn the mapping from label name to ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893797d5-5659-43f9-a9b4-172e1c0f0617",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c4c70d-39f7-45c8-b30a-11dd710d6e2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([all_labels])\n",
    "mlb.transform([[\"tokenization\", \"new model\"], [\"pytorch\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f72dd9c-4a96-45a0-aa03-3a641f92e7f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create splits iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b3c9ee-35af-4060-b6dc-d2cc27b7ccf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee07ab56-1c95-443f-b14d-2d4ba9ad43d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "def balanced_split(df, test_size=0.5):\n",
    "    ind = np.expand_dims(np.arange(len(df)), axis=1)\n",
    "    labels = mlb.transform(df[\"labels\"])\n",
    "    ind_train, _, ind_test, _ = iterative_train_test_split(ind, labels,\n",
    "                                                           test_size)\n",
    "    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0490117a-9549-4074-8073-870ea76b6ce7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_clean = df[[\"text\", \"labels\", \"split\"]].reset_index(drop=True).copy()\n",
    "\n",
    "# unsupervised set\n",
    "df_unsup = df_clean.loc[df_clean[\"split\"] == \"unlabeled\", [\"text\", \"labels\"]]\n",
    "\n",
    "# supervised set\n",
    "df_sup = df_clean.loc[df_clean[\"split\"] == \"labeled\", [\"text\", \"labels\"]]\n",
    "\n",
    "np.random.seed(0)\n",
    "df_train, df_tmp = balanced_split(df_sup, test_size=0.5)\n",
    "df_valid, df_test = balanced_split(df_tmp, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d605431-afde-475b-b19f-093b81d44017",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create a dataset so all these splits are in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d9b3ab6-e7fc-412b-baf3-a52da1b6bd84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train.reset_index(drop=True)),\n",
    "    \"valid\": Dataset.from_pandas(df_valid.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(df_test.reset_index(drop=True)),\n",
    "    \"unsup\": Dataset.from_pandas(df_unsup.reset_index(drop=True))})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a0944e6-3885-4454-ba91-6c67dc3d8d25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create training slides to investigate what's the right balance of supervised to unsupervised data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e4f43c-2a7f-4cde-9686-19ba956d85a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds[\"train\"][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0a3445-125e-47e0-a5f2-5240329a2c31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "all_indices = np.expand_dims(list(range(len(ds[\"train\"]))), axis=1)\n",
    "indices_pool = all_indices\n",
    "labels = mlb.transform(ds[\"train\"][\"labels\"])\n",
    "train_samples = [8, 16, 32, 64, 128]\n",
    "train_slices, last_k = [], 0\n",
    "\n",
    "for i, k in enumerate(train_samples):\n",
    "    # Split off samples necessary to fill the gap to the next split size\n",
    "    indices_pool, labels, new_slice, _ = iterative_train_test_split(\n",
    "        indices_pool, labels, (k-last_k)/len(labels))\n",
    "    last_k = k\n",
    "    if i==0: train_slices.append(new_slice)\n",
    "    else: train_slices.append(np.concatenate((train_slices[-1], new_slice)))\n",
    "\n",
    "# Add full dataset as last slice\n",
    "train_slices.append(all_indices), train_samples.append(len(ds[\"train\"]))\n",
    "train_slices = [np.squeeze(train_slice) for train_slice in train_slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee8025c0-ceae-4cd2-829a-387e38f90eea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Target split sizes:\")\n",
    "print(train_samples)\n",
    "print(\"Actual split sizes:\")\n",
    "print([len(x) for x in train_slices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40cf5ce6-938b-4380-a6eb-f976ceb69dd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Naive Bayesline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f01c5a9-3e48-405b-a4ef-8c1441246d83",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_labels(batch):\n",
    "    batch[\"label_ids\"] = mlb.transform(batch[\"labels\"])\n",
    "    return batch\n",
    "\n",
    "ds = ds.map(prepare_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bee32eb0-f909-40ef-92f2-66a0e7225629",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "macro_scores, micro_scores = defaultdict(list), defaultdict(list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c63eea7-2a04-416a-ae7d-71a54f89c8ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82744b8b-6f92-4de3-b05c-aea32f6db9e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "for train_slice in train_slices:\n",
    "    # Get training slice and test data\n",
    "    ds_train_sample = ds[\"train\"].select(train_slice)\n",
    "    y_train = np.array(ds_train_sample[\"label_ids\"])\n",
    "    y_test = np.array(ds[\"test\"][\"label_ids\"])\n",
    "    # Use a simple count vectorizer to encode our texts as token counts\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(ds_train_sample[\"text\"])\n",
    "    X_test_counts = count_vect.transform(ds[\"test\"][\"text\"])\n",
    "    # Create and train our model!\n",
    "    classifier = BinaryRelevance(classifier=MultinomialNB())\n",
    "    classifier.fit(X_train_counts, y_train)\n",
    "    # Generate predictions and evaluate\n",
    "    y_pred_test = classifier.predict(X_test_counts)\n",
    "    clf_report = classification_report(\n",
    "        y_test, y_pred_test, target_names=mlb.classes_, zero_division=0,\n",
    "        output_dict=True)\n",
    "    # Store metrics\n",
    "    macro_scores[\"Naive Bayes\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
    "    micro_scores[\"Naive Bayes\"].append(clf_report[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a317d30f-927e-433a-996d-5c4dcfd84d72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "    for run in micro_scores.keys():\n",
    "        if run == current_model:\n",
    "            ax0.plot(sample_sizes, micro_scores[run], label=run, linewidth=2)\n",
    "            ax1.plot(sample_sizes, macro_scores[run], label=run, linewidth=2)\n",
    "        else:\n",
    "            ax0.plot(sample_sizes, micro_scores[run], label=run,\n",
    "                     linestyle=\"dashed\")\n",
    "            ax1.plot(sample_sizes, macro_scores[run], label=run,\n",
    "                     linestyle=\"dashed\")\n",
    "\n",
    "    ax0.set_title(\"Micro F1 scores\")\n",
    "    ax1.set_title(\"Macro F1 scores\")\n",
    "    ax0.set_ylabel(\"Test set F1 score\")\n",
    "    ax0.legend(loc=\"lower right\")\n",
    "    for ax in [ax0, ax1]:\n",
    "        ax.set_xlabel(\"Number of training samples\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xticks(sample_sizes)\n",
    "        ax.set_xticklabels(sample_sizes)\n",
    "        ax.minorticks_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "065b9372-194f-460d-b439-bf3af4cf6292",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Plot baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41e77e2-15d1-4983-b026-d56062743116",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a5b9903-a842-4e6b-a7c3-bf55892f1da2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that we plot the number of samples on a logarithmic scale. From the figure we can see that the micro and macro F1-scores both improve as we increase the number of training samples. With so few samples to train on, the results are also slightly noisy since each slice can have a different class distribution. Nevertheless, what’s important here is the trend, so let’s now see how these results fare against transformer-based approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea3fdbe5-85dd-496e-9890-364f225aa9f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Zero Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a08a2f-fb59-4e2f-af99-7c10b4a0ea70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"zero-shot-classification\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1d3f400-9da7-48f1-bc4e-33b6d31bb9f6",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "### Function to predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac6f184-986a-4285-9108-b334e6f25964",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_shot_pipeline(example):\n",
    "    output = pipe(example[\"text\"], all_labels, multi_label=True)\n",
    "    example[\"predicted_labels\"] = output[\"labels\"]\n",
    "    example[\"scores\"] = output[\"scores\"]\n",
    "    return example\n",
    "\n",
    "ds_zero_shot = ds[\"valid\"].map(zero_shot_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b7d084-6009-4075-aabf-33a5df5d9d18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds_zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0e4169-0384-48d6-b6be-11b238579979",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds_zero_shot[\"predicted_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc2e93c2-51ac-436b-b7ec-32ed37069930",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds_zero_shot[\"pred_label_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ae050d5-169c-4d58-8ae4-3958b6fae0e0",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4e2c24-dc3a-41f2-bf85-c3284c0e4a17",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_preds(example, threshold=None, topk=None):\n",
    "    preds = []\n",
    "    if threshold:\n",
    "        for label, score in zip(example[\"predicted_labels\"], example[\"scores\"]):\n",
    "            if score >= threshold:\n",
    "                preds.append(label)\n",
    "    elif topk:\n",
    "        for i in range(topk):\n",
    "            preds.append(example[\"predicted_labels\"][i])\n",
    "    else:\n",
    "        raise ValueError(\"Set either `threshold` or `topk`.\")\n",
    "    return {\"pred_label_ids\": list(np.squeeze(mlb.transform([preds])))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a51f18-883c-4ce5-8a30-03b5e9f534bc",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_clf_report(ds):\n",
    "    y_true = np.array(ds[\"label_ids\"])\n",
    "    y_pred = np.array(ds[\"pred_label_ids\"])\n",
    "    return classification_report(\n",
    "        y_true, y_pred, target_names=mlb.classes_, zero_division=0,\n",
    "        output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347f2745-a27b-49d8-a5fb-1b770f4f7c65",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "macros, micros = [], []\n",
    "topks = [1, 2, 3, 4]\n",
    "for topk in topks:\n",
    "    ds_zero_shot = ds_zero_shot.map(get_preds, batched=False,\n",
    "                                    fn_kwargs={'topk': topk})\n",
    "    clf_report = get_clf_report(ds_zero_shot)\n",
    "    micros.append(clf_report['micro avg']['f1-score'])\n",
    "    macros.append(clf_report['macro avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1506dd64-a2d6-48fa-9b07-325773e285c3",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(topks, micros, label='Micro F1')\n",
    "plt.plot(topks, macros, label='Macro F1')\n",
    "plt.xlabel(\"Top-k\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c389cac9-a977-4971-aa0d-fa1dda1c5c16",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "macros, micros = [], []\n",
    "thresholds = np.linspace(0.01, 1, 100)\n",
    "for threshold in thresholds:\n",
    "    ds_zero_shot = ds_zero_shot.map(get_preds,\n",
    "                                    fn_kwargs={\"threshold\": threshold})\n",
    "    clf_report = get_clf_report(ds_zero_shot)\n",
    "    micros.append(clf_report[\"micro avg\"][\"f1-score\"])\n",
    "    macros.append(clf_report[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd4eabb-f4f3-4d7c-a1f1-5830207ff005",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, micros, label=\"Micro F1\")\n",
    "plt.plot(thresholds, macros, label=\"Macro F1\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f068b652-1a4d-4e50-b8c4-85a832f9602b",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_t, best_micro = thresholds[np.argmax(micros)], np.max(micros)\n",
    "print(f'Best threshold (micro): {best_t} with F1-score {best_micro:.2f}.')\n",
    "best_t, best_macro = thresholds[np.argmax(macros)], np.max(macros)\n",
    "print(f'Best threshold (micro): {best_t} with F1-score {best_macro:.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2115387d-0c13-4fb3-8c72-ca9fa983b1d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This approach fares somewhat worse than the top-1 results, but we can see the precision/recall trade-off clearly in this graph. If we set the threshold too low, then there are too many predictions, which leads to a low precision. If we set the threshold too high, then we will make hardly any predictions, which produces a low recall. From the plot we can see that a threshold value of around 0.8 is the sweet spot between the two.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0024f69d-b62d-442d-a181-0b1779eb1a8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Compare how this performed to NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962e183f-f20f-4188-80fc-0785771e9380",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_zero_shot = ds['test'].map(zero_shot_pipeline)\n",
    "ds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs={'topk': 1})\n",
    "clf_report = get_clf_report(ds_zero_shot)\n",
    "for train_slice in train_slices:\n",
    "    macro_scores['Zero Shot'].append(clf_report['macro avg']['f1-score'])\n",
    "    micro_scores['Zero Shot'].append(clf_report['micro avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04df8167-1f56-4536-8f35-ed2ad99cb08a",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores, macro_scores, train_samples, \"Zero Shot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c3ee1f6-7d31-46d6-843e-0b0a83995127",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Comparing the zero-shot pipeline to the baseline, we observe two things:\n",
    "\n",
    "If we have less than 50 labeled samples, the zero-shot pipeline handily outperforms the baseline.\n",
    "\n",
    "Even above 50 samples, the performance of the zero-shot pipeline is superior when considering both the micro and macro F1-scores. The results for the micro F1-score tell us that the baseline performs well on the frequent classes, while the zero-shot pipeline excels at those since it does not require any examples to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cddd1ff-ebcf-4d93-8652-e969587a8453",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c087baf1-9418-4787-9796-addc49ec4ecd",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "set_seed(3)\n",
    "aug = naw.ContextualWordEmbsAug(model_path=\"distilbert-base-uncased\",\n",
    "                                device=\"cpu\", action=\"substitute\")\n",
    "\n",
    "text = \"Transformers are the most popular toys\"\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Augmented text: {aug.augment(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70348282-6e5d-4e67-94e6-c13fc0f97886",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "## Function to transformer texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6419c46-8d5c-4d68-9d64-446f904e4550",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_text(batch, transformations_per_example=1):\n",
    "    text_aug, label_ids = [], []\n",
    "    for text, labels in zip(batch[\"text\"], batch[\"label_ids\"]):\n",
    "        text_aug += [text]\n",
    "        label_ids += [labels]\n",
    "        for _ in range(transformations_per_example):\n",
    "            text_aug += [aug.augment(text)]\n",
    "            label_ids += [labels]\n",
    "    return {\"text\": text_aug, \"label_ids\": label_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0035826-f995-4125-a718-2d17958bdb36",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "#### Augment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a178c89-8da7-4907-91c8-9976140ba1e8",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = ds[\"train\"].select([0,1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95e2908b-71c9-4023-a40c-69e9c2968844",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_aug, label_ids = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cffd13b-885d-4e06-bdfa-dd9ed00affea",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b0482e1-a47e-4deb-9d25-bdbe5198f7b9",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for text,labels in zip(ds_train_sample[\"text\"], ds_train_sample[\"label_ids\"]):\n",
    "    text_aug += [text]\n",
    "    label_ids += [labels]\n",
    "    for _ in range(1):\n",
    "        text_aug += [aug.augment(text)]\n",
    "        label_ids += [labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f967b56-5ae7-40ec-9b46-ab7a945b1996",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train_sample[\"text\"] = text_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c39bde4-e73c-4231-ab92-685fe9b01a6b",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train_sample2 = ds[\"train\"].select([0,1]).map(augment_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "374b59ac-94de-4eb3-b413-3e81be2991e4",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes + Aug\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a6dd02f-aaa6-48b9-aa71-594fd1df0c0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Use embeddings as a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8963150-ad13-407a-aae3-4fcc0c7eec90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fed7016e-4441-459f-acfc-012990f98871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"miguelvictor/python-gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    # Extract the token embeddings\n",
    "    token_embeddings = model_output[0]\n",
    "    # Compute the attention mask\n",
    "    input_mask_expanded = (attention_mask\n",
    "                           .unsqueeze(-1)\n",
    "                           .expand(token_embeddings.size())\n",
    "                           .float())\n",
    "    # Sum the embeddings, but ignore masked tokens\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    # Return the average as a single vector\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def embed_text(examples):\n",
    "    inputs = tokenizer(examples[\"text\"], padding=True, truncation=True,\n",
    "                       max_length=128, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**inputs)\n",
    "    pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "    return {\"embedding\": pooled_embeds.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff661864-2c4d-4cd1-9a0e-4c51ca45699f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Get embedding for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e37872a-a35d-45b1-8910-e7872d4503ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#embs_train = ds[\"train\"].map(embed_text, batched=True, batch_size=16)\n",
    "#embs_valid = ds[\"valid\"].map(embed_text, batched=True, batch_size=16)\n",
    "embs_test = ds[\"test\"].map(embed_text, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d5f677-2bf8-4bf1-be01-d704d0e04f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Write to pickle to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b421d48-c601-47c5-816e-31fac480b453",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3a110c-771f-44aa-ac10-7ccf233da5e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_file = open(\"embs_train\", \"ab\")\n",
    "valid_file = open(\"embs_valid\", \"ab\")\n",
    "test_file = open(\"embs_test\", \"ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a04fff1-3812-4f02-96f1-d0d95d6e4502",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(embs_train, train_file)\n",
    "pickle.dump(embs_valid, valid_file)\n",
    "pickle.dump(embs_test, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74197eb3-6384-4649-b64b-d96eab6109e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### For when we want to load data back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14987c10-9e4f-4334-b1da-2c8ba65ad4e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_file = open(\"embs_train\", \"rb\")\n",
    "valid_file = open(\"embs_valid\", \"rb\")\n",
    "test_file = open(\"embs_test\", \"rb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf50a3cf-5c9d-4b31-aab4-631d50272784",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embs_train = pickle.load(train_file)\n",
    "embs_valid = pickle.load(valid_file)\n",
    "embs_test = pickle.load(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcc53dc5-0859-4908-b7f6-ebdde4b0f978",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "train_file = open(\"embs_train\", \"rb\")\n",
    "valid_file = open(\"embs_valid\", \"rb\")\n",
    "test_file = open(\"embs_test\", \"rb\")\n",
    "embs_train = pickle.load(train_file)\n",
    "embs_valid = pickle.load(valid_file)\n",
    "embs_test = pickle.load(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74a7a1c7-425b-42ec-8d0b-b2a69cb05e11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Install Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "164c219d-5bfe-4ebf-b9dd-94e76cd33c5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6f4c70c-7024-45a7-9714-68a84bf00d2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a137f01d-ea5f-4cc5-844b-c04df86bd7a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Add embedding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4390c15e-c355-49ee-a04a-6a007b3c0d6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embs_train.add_faiss_index(\"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "451949c7-cfdb-4f95-8324-c575e16ba604",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Perform nearest neighbor lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d6d930a-ef21-4d9f-9824-a3b95b425399",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We need to specify the query embedding as well as the number of nearest neighbors to retrieve. Let’s give it a spin and have a look at the documents that are closest to an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa1d5867-2ad1-4375-acf9-1f9a98453182",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "i, k = 0, 3 # Select the first query and 3 nearest neighbors\n",
    "rn, nl = \"\\r\\n\\r\\n\", \"\\n\" # Used to remove newlines in text for compact display\n",
    "\n",
    "query =  np.array(embs_valid[i][\"embedding\"], dtype=np.float32)\n",
    "scores, samples = embs_train.get_nearest_examples(\"embedding\", query, k=k)\n",
    "\n",
    "print(f\"QUERY LABELS: {embs_valid[i]['labels']}\")\n",
    "print(f\"QUERY TEXT:\\n{embs_valid[i]['text'][:200].replace(rn, nl)} [...]\\n\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Retrieved documents:\")\n",
    "for score, label, text in zip(scores, samples[\"labels\"], samples[\"text\"]):\n",
    "    print(\"=\"*50)\n",
    "    print(f\"TEXT:\\n{text[:200].replace(rn, nl)} [...]\")\n",
    "    print(f\"SCORE: {score:.2f}\")\n",
    "    print(f\"LABELS: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54908cb8-c712-4621-a149-983cc14f45b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Find what's the best k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbcc136a-2ba2-4e91-82ef-d5b1be51815b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_sample_preds(sample, m):\n",
    "    return (np.sum(sample[\"label_ids\"], axis=0) >= m).astype(int)\n",
    "\n",
    "def find_best_k_m(ds_train, valid_queries, valid_labels, max_k=17):\n",
    "    max_k = min(len(ds_train), max_k)\n",
    "    perf_micro = np.zeros((max_k, max_k))\n",
    "    perf_macro = np.zeros((max_k, max_k))\n",
    "    for k in range(1, max_k):\n",
    "        for m in range(1, k + 1):\n",
    "            _, samples = ds_train.get_nearest_examples_batch(\"embedding\",\n",
    "                                                             valid_queries, k=k)\n",
    "            y_pred = np.array([get_sample_preds(s, m) for s in samples])\n",
    "            clf_report = classification_report(valid_labels, y_pred,\n",
    "                target_names=mlb.classes_, zero_division=0, output_dict=True)\n",
    "            perf_micro[k, m] = clf_report[\"micro avg\"][\"f1-score\"]\n",
    "            perf_macro[k, m] = clf_report[\"macro avg\"][\"f1-score\"]\n",
    "    return perf_micro, perf_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fab4a3fb-1b88-4a7b-8ca8-26faa9a5a198",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "valid_labels = np.array(embs_valid[\"label_ids\"])\n",
    "valid_queries = np.array(embs_valid[\"embedding\"], dtype=np.float32)\n",
    "perf_micro, perf_macro = find_best_k_m(embs_train, valid_queries, valid_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "455aaa88-c2ba-40f3-90a9-acada7b7e641",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "k = number of neighbors\n",
    "m = assign all labels that occurred at least m times (m < k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94530fff-a459-4a35-8d62-bdca97d4bcd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "ax0.imshow(perf_micro)\n",
    "ax1.imshow(perf_macro)\n",
    "\n",
    "ax0.set_title(\"micro scores\")\n",
    "ax0.set_ylabel(\"k\")\n",
    "ax1.set_title(\"macro scores\")\n",
    "for ax in [ax0, ax1]:\n",
    "    ax.set_xlim([0.5, 17 - 0.5])\n",
    "    ax.set_ylim([17 - 0.5, 0.5])\n",
    "    ax.set_xlabel(\"m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc30d0d0-2e17-4632-9998-54c148ed8c01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\n",
    "print(f\"Best k: {k}, best m: {m} The perfomance is best when we choose and, or in other words when we retrieve the 15 nearest neighbors and then assign the labels that occurred at least 5 times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f75cda33-49e1-4b57-b150-c97d7a2bf78b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Remove the fiass embedding so we can compare to NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c0329c1-e1d3-49a9-a75f-6b025057fbbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embs_train.drop_index(\"embedding\")\n",
    "test_labels = np.array(embs_test[\"label_ids\"])\n",
    "test_queries = np.array(embs_test[\"embedding\"], dtype=np.float32)\n",
    "\n",
    "for train_slice in train_slices:\n",
    "    # Create a Faiss index from training slice\n",
    "    embs_train_tmp = embs_train.select(train_slice)\n",
    "    embs_train_tmp.add_faiss_index(\"embedding\")\n",
    "    # Get best k, m values with validation set\n",
    "    perf_micro, _ = find_best_k_m(embs_train_tmp, valid_queries, valid_labels)\n",
    "    k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\n",
    "    # Get predictions on test set\n",
    "    _, samples = embs_train_tmp.get_nearest_examples_batch(\"embedding\",\n",
    "                                                           test_queries,\n",
    "                                                           k=int(k))\n",
    "    y_pred = np.array([get_sample_preds(s, m) for s in samples])\n",
    "    # Evaluate predictions\n",
    "    clf_report = classification_report(test_labels, y_pred,\n",
    "        target_names=mlb.classes_, zero_division=0, output_dict=True,)\n",
    "    macro_scores[\"Embedding\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
    "    micro_scores[\"Embedding\"].append(clf_report[\"micro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24c01bc6-c8f3-4c86-a723-ee61d9ef54e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores, macro_scores, train_samples, \"Embedding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515fcaf1-a4c0-41a0-ba5a-1881b9e653da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Fine-tune Vanilla Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db5d43ca-526c-4751-8387-5a4c6b48e7c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Run the def label_id section from NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "400512c8-71ca-4206-a98e-e3bc1b2efe4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "066599e4-5807-4354-b521-91101eed5e53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer, AutoConfig,\n",
    "                          AutoModelForSequenceClassification)\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=128)\n",
    "ds_enc = ds.map(tokenize, batched=True)\n",
    "ds_enc = ds_enc.remove_columns(['labels', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd010cff-3aec-40d0-ba55-dc1983b2542b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First, we create a new column with the labels. The format of that column is inferred from the first element. Then we delete the original column and rename the new one to take the place of the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6da6d255-ea5a-4b8f-b7ef-2f3751e1a21b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds_enc.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "935d2de9-b4e5-413b-9231-ec1234212f50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds_enc = ds_enc.map(lambda x: {\"label_ids_f\": x[\"label_ids\"].to(torch.float)},\n",
    "                    remove_columns=[\"label_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69795057-72df-4217-a298-02f6f3e157d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds_enc = ds_enc.rename_column(\"label_ids_f\", \"label_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e8fa6c4-e4df-41b2-b225-98f9ebefcd10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args_fine_tune = TrainingArguments(\n",
    "    output_dir=\"./results\", num_train_epochs=20, learning_rate=3e-5,\n",
    "    lr_scheduler_type='constant', per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=32, weight_decay=0.0,\n",
    "    evaluation_strategy=\"epoch\", save_strategy=\"epoch\",logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True, metric_for_best_model='micro f1',\n",
    "    save_total_limit=1, log_level='error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69f90525-ba55-4250-97bb-0b54b74c4ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Get F1 score to choose the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afc431a0-f05d-4c0c-bfb7-feba4ed5c764",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    y_true = pred.label_ids\n",
    "    y_pred = sigmoid(pred.predictions)\n",
    "    y_pred = (y_pred>0.5).astype(float)\n",
    "\n",
    "    clf_dict = classification_report(y_true, y_pred, target_names=all_labels,\n",
    "                                     zero_division=0, output_dict=True)\n",
    "    return {\"micro f1\": clf_dict[\"micro avg\"][\"f1-score\"],\n",
    "            \"macro f1\": clf_dict[\"macro avg\"][\"f1-score\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd4ddf21-9c92-48fa-8a04-57e4b65e5b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "config.num_labels = len(all_labels)\n",
    "config.problem_type = \"multi_label_classification\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe685d0-cc6e-4cb2-aa07-7f1649db1249",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for train_slice in train_slices:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,\n",
    "                                                               config=config)\n",
    "    trainer = Trainer(\n",
    "        model=model, tokenizer=tokenizer,\n",
    "        args=training_args_fine_tune,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=ds_enc[\"train\"].select(train_slice),\n",
    "        eval_dataset=ds_enc[\"valid\"],)\n",
    "\n",
    "    trainer.train()\n",
    "    pred = trainer.predict(ds_enc[\"test\"])\n",
    "    metrics = compute_metrics(pred)\n",
    "    macro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"macro f1\"])\n",
    "    micro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"micro f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f0ad78-ffa8-4818-b5db-c6c85ea8bfef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (vanilla)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8fe2866-c3c9-420a-a1c8-451cb1b09fb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b081f0ce-93f4-4607-b279-eb0e8f18bd38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this step we use the classic language model objective of predicting masked words, which means we don’t need any labeled data. After that we can load the adapted model as a classifier and fine-tune it, thus leveraging the unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fcfaea8-5cf6-4346-81d6-a5d5868759a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start by fine-tuning a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f069fca-748d-44c1-89c8-1d2b41d8f516",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "we’ll fine-tune the pretrained BERT model with masked language modeling on the unlabeled portion of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b6ba67c-8707-4f31-b25f-78dabb609810",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Re-tokenize text so it doesn't predict [CLS] or [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbebaed7-1e98-4227-b6b1-43a15fa295a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True,\n",
    "                     max_length=128, return_special_tokens_mask=True)\n",
    "\n",
    "ds_mlm = ds.map(tokenize, batched=True)\n",
    "ds_mlm = ds_mlm.remove_columns([\"labels\", \"text\", \"label_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd6d0273-4449-4fbc-9ca2-8a6a53664999",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Use data-collator to prepare the elements in the bach to feed them to the model. We need to mask tokens in the input sequence and have the target tokens in the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ea16b33-8431-42c1-a172-6be6f0621151",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, set_seed\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd31ee4c-3778-4022-be0d-0bc61bb1de54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example of what data collator is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0fc6569-cf29-4245-b84d-38376bdd9d86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_collator.return_tensors = \"pt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c69ad2-f45c-425d-ba30-7b118d8fb3c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Need to login to huggingface first through terminal! Run `hugging-face cli login`\n",
    "\n",
    "Use the write tokens from here: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2014a22-cb33-4509-be41-270aaabfa7b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"{model_ckpt}-issues-128\", per_device_train_batch_size=32,\n",
    "    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\", save_strategy=\"no\",\n",
    "    num_train_epochs=16, push_to_hub=True, log_level=\"error\", report_to=\"none\")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\"),\n",
    "        tokenizer=tokenizer, args=training_args, data_collator=data_collator,\n",
    "        train_dataset=ds_mlm[\"unsup\"], eval_dataset=ds_mlm[\"train\"])\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d6236fd-e507-47a8-82e8-e9a3b6706360",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307c0292-537e-43a5-8453-105a9264fb63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_file = open(\"trainer\", \"ab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7601a426-b08d-4f9a-a9e5-727664c0b30d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(trainer, train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d1a67b-cc81-4bc4-854b-e58986c50f8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1de6336a-474f-4697-bdaa-9de455e8ce7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_log = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "(df_log.dropna(subset=[\"eval_loss\"]).reset_index()[\"eval_loss\"]\n",
    " .plot(label=\"Validation\"))\n",
    "df_log.dropna(subset=[\"loss\"]).reset_index()[\"loss\"].plot(label=\"Train\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4553d5ff-77fd-4782-a186-03db47e07140",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fine-Tuning Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89b2b1bd-f81e-4631-a2ca-7a42ca241c30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_ckpt = f'{model_ckpt}-issues-128'\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "config.num_labels = len(all_labels)\n",
    "config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "for train_slice in train_slices:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,\n",
    "                                                               config=config)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args_fine_tune,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=ds_enc[\"train\"].select(train_slice),\n",
    "        eval_dataset=ds_enc[\"valid\"],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    pred = trainer.predict(ds_enc['test'])\n",
    "    metrics = compute_metrics(pred)\n",
    "    # DA refers to domain adaptation\n",
    "    macro_scores['Fine-tune (DA)'].append(metrics['macro f1'])\n",
    "    micro_scores['Fine-tune (DA)'].append(metrics['micro f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41fb2ec9-a257-4540-b644-8b920d7ff5a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (DA)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "531d361a-e600-40f9-8805-61f637e8f43f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "github_example",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
